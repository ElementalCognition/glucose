{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate_GLUCOSE_submissions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNlw_oNsp_gD"
      },
      "source": [
        "### This script generates the automatic metric scores for all the model submissions (in the canonical GLUCOSE test set format)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlPz1paDPFb7",
        "outputId": "8d0d33ae-f593-4758-83ff-b20164512c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pandas\n",
        "!pip3 install sacrebleu"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.4.14)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfmLifEBjuZ5"
      },
      "source": [
        "import sacrebleu"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOHGhXoHMq1h"
      },
      "source": [
        "import glob, os\n",
        "import re\n",
        "import pandas\n",
        "from collections import defaultdict"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-xHm-UlOwn7",
        "outputId": "8991884b-7650-4c8b-b29b-427392491f87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This block can be commented out if Google Drive is not used\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxnewSicHZJt"
      },
      "source": [
        "## Set the paths\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4khMHxOqhsC"
      },
      "source": [
        "# Note: change the following paths to point to your model submission folders and the GLUCOSE 'test_set_answer_key.csv' file.\n",
        "submissions_path = \"/content/drive/My Drive/GLUCOSE Colab Notebooks/submissions/\"\n",
        "key_file_path = \"/content/drive/My Drive/GLUCOSE Colab Notebooks/test_set/test_set_answer_key\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVNz26EVqoqf"
      },
      "source": [
        "## Calculate the metrics\n",
        "We are only caclulating BLEU scores here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIb-3ao2TJz6"
      },
      "source": [
        "model_evaluation_results = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNzgVTZdDwOm"
      },
      "source": [
        "df_key = pandas.read_csv(key_file_path, encoding=\"utf-8\")\n",
        "for path in glob.glob(os.path.join('', submissions_path+\"*.csv\")):\n",
        "  model_name = path.replace(submissions_path,\"\").replace(\".csv\", \"\")\n",
        "  df_generation = pandas.read_csv(path, encoding=\"utf-8\")\n",
        "  for dim in range(10):\n",
        "    dim = dim + 1\n",
        "    for mode in [\"specific\", \"general\"]:\n",
        "      predictions = df_generation[str(dim)+\"_\"+mode+\"NL\"].values\n",
        "      references = [[], [], []] #3 references per each test case\n",
        "      for one_row in df_key[str(dim)+\"_\"+mode+\"NL\"].values:\n",
        "        if one_row != \"escaped\":\n",
        "          splitted = one_row.split(\"****\")\n",
        "          for i in range(3):            \n",
        "            references[i].append(splitted[i])\n",
        "        else:\n",
        "          for i in range(3):\n",
        "            references[i].append(\"escaped\")\n",
        "      bleu = sacrebleu.corpus_bleu(predictions, references).score\n",
        "      model_evaluation_results[\"dim_\"+str(dim)][mode][model_name][\"bleu\"] = bleu"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Gg_fLJHlUm"
      },
      "source": [
        "## Print the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf5C320kAbWR",
        "outputId": "29ed6319-1daf-4a66-9fc9-75cdaf6e03b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_evaluation_results"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>>, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}